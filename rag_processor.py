import os
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings  # ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader, UnstructuredMarkdownLoader
from config import DATA_DIR, CHUNK_SIZE, CHUNK_OVERLAP, EMBEDDINGS_MODEL, VECTOR_DB_PATH

# class RAGProcessor:
#     def __init__(self):
#         # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸ĞµĞ¹ HuggingFaceEmbeddings
#         self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)
#         self.vector_db = None
        
#     def _ensure_vector_db_dir(self):
#         """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ‘Ğ”"""
#         os.makedirs(VECTOR_DB_PATH, exist_ok=True)
#         return VECTOR_DB_PATH
        
#     def load_and_process_documents(self):
#         """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹"""
#         if not os.path.exists(DATA_DIR):
#             os.makedirs(DATA_DIR, exist_ok=True)
#             print(f"ĞŸĞ°Ğ¿ĞºĞ° {DATA_DIR} ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°. Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹.")
#             return
        
#         db_path = self._ensure_vector_db_dir()
        
#         # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ÑƒÑ Ğ‘Ğ” Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸
#         try:
#             if os.path.exists(db_path / "index.faiss"):
#                 self.vector_db = FAISS.load_local(
#                     db_path, 
#                     self.embeddings,
#                     allow_dangerous_deserialization=True  # Ğ Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµĞ¼ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ pickle
#                 )
#                 return
#         except Exception as e:
#             print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ‘Ğ”: {e}. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ½Ğ¾Ğ²ÑƒÑ.")
            
#         # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
#         documents = []
#         for root, _, files in os.walk(DATA_DIR):
#             for file in files:
#                 file_path = Path(root) / file
#                 try:
#                     if file.endswith('.md'):
#                         loader = UnstructuredMarkdownLoader(str(file_path))
#                     else:
#                         loader = TextLoader(str(file_path), encoding='utf-8')
#                     documents.extend(loader.load())
#                 except Exception as e:
#                     print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ {file_path}: {e}")
#                     continue
        
#         if not documents:
#             print("ĞĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸.")
#             return
            
#         # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡Ğ°Ğ½ĞºĞ¸
#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=CHUNK_SIZE,
#             chunk_overlap=CHUNK_OVERLAP
#         )
#         chunks = text_splitter.split_documents(documents)
        
#         # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ‘Ğ”
#         try:
#             self.vector_db = FAISS.from_documents(chunks, self.embeddings)
#             self.vector_db.save_local(db_path)
#             print(f"Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ² {db_path}")
#         except Exception as e:
#             print(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ: {e}")
#             self.vector_db = FAISS.from_documents(chunks, self.embeddings)
    
#     def search_relevant_documents(self, query: str, k: int = 3):
#         """ĞŸĞ¾Ğ¸ÑĞº Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"""
#         return [doc.page_content for doc in self.vector_db.similarity_search(query, k=k)] if self.vector_db else []
    



import re
from typing import List, Dict
from langchain.docstore.document import Document


class LawTextProcessor:
    @staticmethod
    def split_by_articles(text: str, law_name: str) -> List[Dict]:
        """Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        pattern = r"(Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ \d+\.?\s*.+?)(?=(Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ \d+|$))"
        articles = re.findall(pattern, text, re.DOTALL)
        
        result = []
        for article in articles:
            article_text = article[0].strip()
            article_num = re.search(r"Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ (\d+\.?\d*)", article_text).group(1)
            result.append({
                "text": article_text,
                "metadata": {
                    "law": law_name,
                    "article": article_num,
                    "source": f"{law_name} Ğ¡Ñ‚. {article_num}"
                }
            })
        return result

class RAGProcessor:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)
        self.vector_db = None

    def load_and_process_documents(self):
        """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ ÑÑ‚Ğ°Ñ‚ÑŒÑĞ¼"""
        if not os.path.exists(DATA_DIR):
            os.makedirs(DATA_DIR)
            return

        documents = []
        for file in os.listdir(DATA_DIR):
            file_path = os.path.join(DATA_DIR, file)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ° Ğ¸Ğ· Ğ¸Ğ¼ĞµĞ½Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ°
                law_name = os.path.splitext(file)[0]
                
                # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞ¸
                articles = LawTextProcessor.split_by_articles(text, law_name)
                
                for article in articles:
                    doc = Document(
                        page_content=article["text"],
                        metadata=article["metadata"]
                    )
                    documents.append(doc)
                    
            except Exception as e:
                print(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ° {file}: {str(e)}")

        if documents:
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP
            )
            chunks = text_splitter.split_documents(documents)
            self.vector_db = FAISS.from_documents(chunks, self.embeddings)
            self.vector_db.save_local(VECTOR_DB_PATH)

    def search_relevant_documents(self, query: str, k: int = 3) -> List[str]:
        if not self.vector_db:
            return []
        
        docs = self.vector_db.similarity_search(query, k=k)
        
        results = []
        for doc in docs:
            law = doc.metadata.get("law", "Ğ—Ğ°ĞºĞ¾Ğ½")
            article = doc.metadata.get("article", "N/A")
            results.append(
                f"ğŸ“– {law}\n"
                f"ğŸ“ Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ {article}\n"
                f"ğŸ“„ {doc.page_content[:300]}..."
            )
        return results